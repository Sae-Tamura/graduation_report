
@misc{lu_visual_2016,
	title = {Visual Relationship Detection with Language Priors},
	url = {http://arxiv.org/abs/1608.00187},
	doi = {10.48550/arXiv.1608.00187},
	abstract = {Visual relationships capture a wide variety of interactions between pairs of objects in images (e.g. “man riding bicycle” and “man pushing bicycle”). Consequently, the set of possible relationships is extremely large and it is diﬃcult to obtain suﬃcient training examples for all possible relationships. Because of this limitation, previous work on visual relationship detection has concentrated on predicting only a handful of relationships. Though most relationships are infrequent, their objects (e.g. “man” and “bicycle”) and predicates (e.g. “riding” and “pushing”) independently occur more frequently. We propose a model that uses this insight to train visual models for objects and predicates individually and later combines them together to predict multiple relationships per image. We improve on prior work by leveraging language priors from semantic word embeddings to ﬁnetune the likelihood of a predicted relationship. Our model can scale to predict thousands of types of relationships from a few examples. Additionally, we localize the objects in the predicted relationships as bounding boxes in the image. We further demonstrate that understanding relationships can improve content based image retrieval.},
	number = {{arXiv}:1608.00187},
	publisher = {{arXiv}},
	author = {Lu, Cewu and Krishna, Ranjay and Bernstein, Michael and Fei-Fei, Li},
	urldate = {2025-12-09},
	date = {2016-07-31},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1608.00187 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:/Users/tamurasae/Zotero/storage/QGBPQP8A/Lu et al. - 2016 - Visual Relationship Detection with Language Priors.pdf:application/pdf},
}

@misc{xu_scene_2017,
	title = {Scene Graph Generation by Iterative Message Passing},
	url = {http://arxiv.org/abs/1701.02426},
	doi = {10.48550/arXiv.1701.02426},
	abstract = {Understanding a visual scene goes beyond recognizing individual objects in isolation. Relationships between objects also constitute rich semantic information about the scene. In this work, we explicitly model the objects and their relationships using scene graphs, a visually-grounded graphical structure of an image. We propose a novel endto-end model that generates such structured scene representation from an input image. The model solves the scene graph inference problem using standard {RNNs} and learns to iteratively improves its predictions via message passing. Our joint inference model can take advantage of contextual cues to make better predictions on objects and their relationships. The experiments show that our model significantly outperforms previous methods for generating scene graphs using Visual Genome dataset and inferring support relations with {NYU} Depth v2 dataset.},
	number = {{arXiv}:1701.02426},
	publisher = {{arXiv}},
	author = {Xu, Danfei and Zhu, Yuke and Choy, Christopher B. and Fei-Fei, Li},
	urldate = {2025-12-09},
	date = {2017-04-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1701.02426 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:/Users/tamurasae/Zotero/storage/ZL5CVHB4/Xu et al. - 2017 - Scene Graph Generation by Iterative Message Passing.pdf:application/pdf},
}

@misc{zellers_neural_2018,
	title = {Neural Motifs: Scene Graph Parsing with Global Context},
	url = {http://arxiv.org/abs/1711.06640},
	doi = {10.48550/arXiv.1711.06640},
	shorttitle = {Neural Motifs},
	abstract = {We investigate the problem of producing structured graph representations of visual scenes. Our work analyzes the role of motifs: regularly appearing substructures in scene graphs. We present new quantitative insights on such repeated structures in the Visual Genome dataset. Our analysis shows that object labels are highly predictive of relation labels but not vice-versa. We also ﬁnd that there are recurring patterns even in larger subgraphs: more than 50\% of graphs contain motifs involving at least two relations. Our analysis motivates a new baseline: given object detections, predict the most frequent relation between object pairs with the given labels, as seen in the training set. This baseline improves on the previous state-of-the-art by an average of 3.6\% relative improvement across evaluation settings. We then introduce Stacked Motif Networks, a new architecture designed to capture higher order motifs in scene graphs that further improves over our strong baseline by an average 7.1\% relative gain. Our code is available at github.com/rowanz/neural-motifs.},
	number = {{arXiv}:1711.06640},
	publisher = {{arXiv}},
	author = {Zellers, Rowan and Yatskar, Mark and Thomson, Sam and Choi, Yejin},
	urldate = {2025-12-09},
	date = {2018-03-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1711.06640 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:/Users/tamurasae/Zotero/storage/LD3RWSGV/Zellers et al. - 2018 - Neural Motifs Scene Graph Parsing with Global Context.pdf:application/pdf},
}

@misc{yang_graph_2018,
	title = {Graph R-{CNN} for Scene Graph Generation},
	url = {http://arxiv.org/abs/1808.00191},
	doi = {10.48550/arXiv.1808.00191},
	abstract = {We propose a novel scene graph generation model called Graph R-{CNN}, that is both eﬀective and eﬃcient at detecting objects and their relations in images. Our model contains a Relation Proposal Network ({RePN}) that eﬃciently deals with the quadratic number of potential relations between objects in an image. We also propose an attentional Graph Convolutional Network ({aGCN}) that eﬀectively captures contextual information between objects and relations. Finally, we introduce a new evaluation metric that is more holistic and realistic than existing metrics. We report state-of-the-art performance on scene graph generation as evaluated using both existing and our proposed metrics.},
	number = {{arXiv}:1808.00191},
	publisher = {{arXiv}},
	author = {Yang, Jianwei and Lu, Jiasen and Lee, Stefan and Batra, Dhruv and Parikh, Devi},
	urldate = {2025-12-09},
	date = {2018-08-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1808.00191 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {PDF:/Users/tamurasae/Zotero/storage/UU9ACNSP/Yang et al. - 2018 - Graph R-CNN for Scene Graph Generation.pdf:application/pdf},
}

@misc{tang_learning_2018,
	title = {Learning to Compose Dynamic Tree Structures for Visual Contexts},
	url = {http://arxiv.org/abs/1812.01880},
	doi = {10.48550/arXiv.1812.01880},
	abstract = {We propose to compose dynamic tree structures that place the objects in an image into a visual context, helping visual reasoning tasks such as scene graph generation and visual Q\&A. Our visual context tree model, dubbed {VCTREE}, has two key advantages over existing structured object representations including chains and fully-connected graphs: 1) The efﬁcient and expressive binary tree encodes the inherent parallel/hierarchical relationships among objects, e.g., “clothes” and “pants” are usually co-occur and belong to “person”; 2) the dynamic structure varies from image to image and task to task, allowing more content/task-speciﬁc message passing among objects. To construct a {VCTREE}, we design a score function that calculates the task-dependent validity between each object pair, and the tree is the binary version of the maximum spanning tree from the score matrix. Then, visual contexts are encoded by bidirectional {TreeLSTM} and decoded by task-speciﬁc models. We develop a hybrid learning procedure which integrates end-task supervised learning and the tree structure reinforcement learning, where the former’s evaluation result serves as a self-critic for the latter’s structure exploration. Experimental results on two benchmarks, which require reasoning over contexts: Visual Genome for scene graph generation and {VQA}2.0 for visual Q\&A, show that {VCTREE} outperforms state-of-the-art results while discovering interpretable visual context structures.},
	number = {{arXiv}:1812.01880},
	publisher = {{arXiv}},
	author = {Tang, Kaihua and Zhang, Hanwang and Wu, Baoyuan and Luo, Wenhan and Liu, Wei},
	urldate = {2025-12-09},
	date = {2018-12-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1812.01880 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:/Users/tamurasae/Zotero/storage/8EJ7IVAJ/Tang et al. - 2018 - Learning to Compose Dynamic Tree Structures for Visual Contexts.pdf:application/pdf},
}

@misc{li_sgtr_2022,
	title = {{SGTR}: End-to-end Scene Graph Generation with Transformer},
	url = {http://arxiv.org/abs/2112.12970},
	doi = {10.48550/arXiv.2112.12970},
	shorttitle = {{SGTR}},
	abstract = {Scene Graph Generation ({SGG}) remains a challenging visual understanding task due to its compositional property. Most previous works adopt a bottom-up two-stage or a point-based one-stage approach, which often suffers from high time complexity or sub-optimal designs. In this work, we propose a novel {SGG} method to address the aforementioned issues, formulating the task as a bipartite graph construction problem. To solve the problem, we develop a transformer-based end-to-end framework that first generates the entity and predicate proposal set, followed by inferring directed edges to form the relation triplets. In particular, we develop a new entity-aware predicate representation based on a structural predicate generator that leverages the compositional property of relationships. Moreover, we design a graph assembling module to infer the connectivity of the bipartite scene graph based on our entity-aware structure, enabling us to generate the scene graph in an end-to-end manner. Extensive experimental results show that our design is able to achieve the state-of-the-art or comparable performance on two challenging benchmarks, surpassing most of the existing approaches and enjoying higher efficiency in inference. We hope our model can serve as a strong baseline for the Transformer-based scene graph generation. Code is available: https://github.com/Scarecrow0/{SGTR}},
	number = {{arXiv}:2112.12970},
	publisher = {{arXiv}},
	author = {Li, Rongjie and Zhang, Songyang and He, Xuming},
	urldate = {2025-12-09},
	date = {2022-03-31},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2112.12970 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {PDF:/Users/tamurasae/Zotero/storage/9ZJWBCZV/Li et al. - 2022 - SGTR End-to-end Scene Graph Generation with Transformer.pdf:application/pdf},
}

@misc{cong_reltr_2023,
	title = {{RelTR}: Relation Transformer for Scene Graph Generation},
	url = {http://arxiv.org/abs/2201.11460},
	doi = {10.48550/arXiv.2201.11460},
	shorttitle = {{RelTR}},
	abstract = {Different objects in the same scene are more or less related to each other, but only a limited number of these relationships are noteworthy. Inspired by Detection Transformer, which excels in object detection, we view scene graph generation as a set prediction problem. In this paper, we propose an end-to-end scene graph generation model Relation Transformer ({RelTR}), which has an encoder-decoder architecture. The encoder reasons about the visual feature context while the decoder infers a ﬁxed-size set of triplets subject-predicate-object using different types of attention mechanisms with coupled subject and object queries. We design a set prediction loss performing the matching between the ground truth and predicted triplets for the end-to-end training. In contrast to most existing scene graph generation methods, {RelTR} is a one-stage method that predicts sparse scene graphs directly only using visual appearance without combining entities and labeling all possible predicates. Extensive experiments on the Visual Genome, Open Images V6, and {VRD} datasets demonstrate the superior performance and fast inference of our model.},
	number = {{arXiv}:2201.11460},
	publisher = {{arXiv}},
	author = {Cong, Yuren and Yang, Michael Ying and Rosenhahn, Bodo},
	urldate = {2025-12-09},
	date = {2023-04-14},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2201.11460 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:/Users/tamurasae/Zotero/storage/6YBTRYHF/Cong et al. - 2023 - RelTR Relation Transformer for Scene Graph Generation.pdf:application/pdf},
}

@misc{im_egtr_2024,
	title = {{EGTR}: Extracting Graph from Transformer for Scene Graph Generation},
	url = {http://arxiv.org/abs/2404.02072},
	doi = {10.48550/arXiv.2404.02072},
	shorttitle = {{EGTR}},
	abstract = {Scene Graph Generation ({SGG}) is a challenging task of detecting objects and predicting relationships between objects. After {DETR} was developed, one-stage {SGG} models based on a one-stage object detector have been actively studied. However, complex modeling is used to predict the relationship between objects, and the inherent relationship between object queries learned in the multi-head self-attention of the object detector has been neglected. We propose a lightweight one-stage {SGG} model that extracts the relation graph from the various relationships learned in the multi-head self-attention layers of the {DETR} decoder. By fully utilizing the self-attention by-products, the relation graph can be extracted effectively with a shallow relation extraction head. Considering the dependency of the relation extraction task on the object detection task, we propose a novel relation smoothing technique that adjusts the relation label adaptively according to the quality of the detected objects. By the relation smoothing, the model is trained according to the continuous curriculum that focuses on object detection task at the beginning of training and performs multi-task learning as the object detection performance gradually improves. Furthermore, we propose a connectivity prediction task that predicts whether a relation exists between object pairs as an auxiliary task of the relation extraction. We demonstrate the effectiveness and efficiency of our method for the Visual Genome and Open Image V6 datasets. Our code is publicly available at https://github.com/naver-ai/egtr.},
	number = {{arXiv}:2404.02072},
	publisher = {{arXiv}},
	author = {Im, Jinbae and Nam, {JeongYeon} and Park, Nokyung and Lee, Hyungmin and Park, Seunghyun},
	urldate = {2025-12-09},
	date = {2024-06-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2404.02072 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {PDF:/Users/tamurasae/Zotero/storage/Y6QGEJX2/Im et al. - 2024 - EGTR Extracting Graph from Transformer for Scene Graph Generation.pdf:application/pdf},
}
