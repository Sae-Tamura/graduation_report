
@misc{lu_visual_2016,
	title = {Visual Relationship Detection with Language Priors},
	url = {http://arxiv.org/abs/1608.00187},
	doi = {10.48550/arXiv.1608.00187},
	abstract = {Visual relationships capture a wide variety of interactions between pairs of objects in images (e.g. “man riding bicycle” and “man pushing bicycle”). Consequently, the set of possible relationships is extremely large and it is diﬃcult to obtain suﬃcient training examples for all possible relationships. Because of this limitation, previous work on visual relationship detection has concentrated on predicting only a handful of relationships. Though most relationships are infrequent, their objects (e.g. “man” and “bicycle”) and predicates (e.g. “riding” and “pushing”) independently occur more frequently. We propose a model that uses this insight to train visual models for objects and predicates individually and later combines them together to predict multiple relationships per image. We improve on prior work by leveraging language priors from semantic word embeddings to ﬁnetune the likelihood of a predicted relationship. Our model can scale to predict thousands of types of relationships from a few examples. Additionally, we localize the objects in the predicted relationships as bounding boxes in the image. We further demonstrate that understanding relationships can improve content based image retrieval.},
	number = {{arXiv}:1608.00187},
	publisher = {{arXiv}},
	author = {Lu, Cewu and Krishna, Ranjay and Bernstein, Michael and Fei-Fei, Li},
	urldate = {2025-12-09},
	date = {2016-07-31},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1608.00187 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:/Users/tamurasae/Zotero/storage/QGBPQP8A/Lu et al. - 2016 - Visual Relationship Detection with Language Priors.pdf:application/pdf},
}

@misc{xu_scene_2017,
	title = {Scene Graph Generation by Iterative Message Passing},
	url = {http://arxiv.org/abs/1701.02426},
	doi = {10.48550/arXiv.1701.02426},
	abstract = {Understanding a visual scene goes beyond recognizing individual objects in isolation. Relationships between objects also constitute rich semantic information about the scene. In this work, we explicitly model the objects and their relationships using scene graphs, a visually-grounded graphical structure of an image. We propose a novel endto-end model that generates such structured scene representation from an input image. The model solves the scene graph inference problem using standard {RNNs} and learns to iteratively improves its predictions via message passing. Our joint inference model can take advantage of contextual cues to make better predictions on objects and their relationships. The experiments show that our model significantly outperforms previous methods for generating scene graphs using Visual Genome dataset and inferring support relations with {NYU} Depth v2 dataset.},
	number = {{arXiv}:1701.02426},
	publisher = {{arXiv}},
	author = {Xu, Danfei and Zhu, Yuke and Choy, Christopher B. and Fei-Fei, Li},
	urldate = {2025-12-09},
	date = {2017-04-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1701.02426 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:/Users/tamurasae/Zotero/storage/ZL5CVHB4/Xu et al. - 2017 - Scene Graph Generation by Iterative Message Passing.pdf:application/pdf},
}

@misc{zellers_neural_2018,
	title = {Neural Motifs: Scene Graph Parsing with Global Context},
	url = {http://arxiv.org/abs/1711.06640},
	doi = {10.48550/arXiv.1711.06640},
	shorttitle = {Neural Motifs},
	abstract = {We investigate the problem of producing structured graph representations of visual scenes. Our work analyzes the role of motifs: regularly appearing substructures in scene graphs. We present new quantitative insights on such repeated structures in the Visual Genome dataset. Our analysis shows that object labels are highly predictive of relation labels but not vice-versa. We also ﬁnd that there are recurring patterns even in larger subgraphs: more than 50\% of graphs contain motifs involving at least two relations. Our analysis motivates a new baseline: given object detections, predict the most frequent relation between object pairs with the given labels, as seen in the training set. This baseline improves on the previous state-of-the-art by an average of 3.6\% relative improvement across evaluation settings. We then introduce Stacked Motif Networks, a new architecture designed to capture higher order motifs in scene graphs that further improves over our strong baseline by an average 7.1\% relative gain. Our code is available at github.com/rowanz/neural-motifs.},
	number = {{arXiv}:1711.06640},
	publisher = {{arXiv}},
	author = {Zellers, Rowan and Yatskar, Mark and Thomson, Sam and Choi, Yejin},
	urldate = {2025-12-09},
	date = {2018-03-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1711.06640 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:/Users/tamurasae/Zotero/storage/LD3RWSGV/Zellers et al. - 2018 - Neural Motifs Scene Graph Parsing with Global Context.pdf:application/pdf},
}

@misc{yang_graph_2018,
	title = {Graph R-{CNN} for Scene Graph Generation},
	url = {http://arxiv.org/abs/1808.00191},
	doi = {10.48550/arXiv.1808.00191},
	abstract = {We propose a novel scene graph generation model called Graph R-{CNN}, that is both eﬀective and eﬃcient at detecting objects and their relations in images. Our model contains a Relation Proposal Network ({RePN}) that eﬃciently deals with the quadratic number of potential relations between objects in an image. We also propose an attentional Graph Convolutional Network ({aGCN}) that eﬀectively captures contextual information between objects and relations. Finally, we introduce a new evaluation metric that is more holistic and realistic than existing metrics. We report state-of-the-art performance on scene graph generation as evaluated using both existing and our proposed metrics.},
	number = {{arXiv}:1808.00191},
	publisher = {{arXiv}},
	author = {Yang, Jianwei and Lu, Jiasen and Lee, Stefan and Batra, Dhruv and Parikh, Devi},
	urldate = {2025-12-09},
	date = {2018-08-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1808.00191 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {PDF:/Users/tamurasae/Zotero/storage/UU9ACNSP/Yang et al. - 2018 - Graph R-CNN for Scene Graph Generation.pdf:application/pdf},
}

@misc{tang_learning_2018,
	title = {Learning to Compose Dynamic Tree Structures for Visual Contexts},
	url = {http://arxiv.org/abs/1812.01880},
	doi = {10.48550/arXiv.1812.01880},
	abstract = {We propose to compose dynamic tree structures that place the objects in an image into a visual context, helping visual reasoning tasks such as scene graph generation and visual Q\&A. Our visual context tree model, dubbed {VCTREE}, has two key advantages over existing structured object representations including chains and fully-connected graphs: 1) The efﬁcient and expressive binary tree encodes the inherent parallel/hierarchical relationships among objects, e.g., “clothes” and “pants” are usually co-occur and belong to “person”; 2) the dynamic structure varies from image to image and task to task, allowing more content/task-speciﬁc message passing among objects. To construct a {VCTREE}, we design a score function that calculates the task-dependent validity between each object pair, and the tree is the binary version of the maximum spanning tree from the score matrix. Then, visual contexts are encoded by bidirectional {TreeLSTM} and decoded by task-speciﬁc models. We develop a hybrid learning procedure which integrates end-task supervised learning and the tree structure reinforcement learning, where the former’s evaluation result serves as a self-critic for the latter’s structure exploration. Experimental results on two benchmarks, which require reasoning over contexts: Visual Genome for scene graph generation and {VQA}2.0 for visual Q\&A, show that {VCTREE} outperforms state-of-the-art results while discovering interpretable visual context structures.},
	number = {{arXiv}:1812.01880},
	publisher = {{arXiv}},
	author = {Tang, Kaihua and Zhang, Hanwang and Wu, Baoyuan and Luo, Wenhan and Liu, Wei},
	urldate = {2025-12-09},
	date = {2018-12-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1812.01880 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:/Users/tamurasae/Zotero/storage/8EJ7IVAJ/Tang et al. - 2018 - Learning to Compose Dynamic Tree Structures for Visual Contexts.pdf:application/pdf},
}

@misc{li_sgtr_2022,
	title = {{SGTR}: End-to-end Scene Graph Generation with Transformer},
	url = {http://arxiv.org/abs/2112.12970},
	doi = {10.48550/arXiv.2112.12970},
	shorttitle = {{SGTR}},
	abstract = {Scene Graph Generation ({SGG}) remains a challenging visual understanding task due to its compositional property. Most previous works adopt a bottom-up two-stage or a point-based one-stage approach, which often suffers from high time complexity or sub-optimal designs. In this work, we propose a novel {SGG} method to address the aforementioned issues, formulating the task as a bipartite graph construction problem. To solve the problem, we develop a transformer-based end-to-end framework that first generates the entity and predicate proposal set, followed by inferring directed edges to form the relation triplets. In particular, we develop a new entity-aware predicate representation based on a structural predicate generator that leverages the compositional property of relationships. Moreover, we design a graph assembling module to infer the connectivity of the bipartite scene graph based on our entity-aware structure, enabling us to generate the scene graph in an end-to-end manner. Extensive experimental results show that our design is able to achieve the state-of-the-art or comparable performance on two challenging benchmarks, surpassing most of the existing approaches and enjoying higher efficiency in inference. We hope our model can serve as a strong baseline for the Transformer-based scene graph generation. Code is available: https://github.com/Scarecrow0/{SGTR}},
	number = {{arXiv}:2112.12970},
	publisher = {{arXiv}},
	author = {Li, Rongjie and Zhang, Songyang and He, Xuming},
	urldate = {2025-12-09},
	date = {2022-03-31},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2112.12970 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {PDF:/Users/tamurasae/Zotero/storage/9ZJWBCZV/Li et al. - 2022 - SGTR End-to-end Scene Graph Generation with Transformer.pdf:application/pdf},
}

@misc{cong_reltr_2023,
	title = {{RelTR}: Relation Transformer for Scene Graph Generation},
	url = {http://arxiv.org/abs/2201.11460},
	doi = {10.48550/arXiv.2201.11460},
	shorttitle = {{RelTR}},
	abstract = {Different objects in the same scene are more or less related to each other, but only a limited number of these relationships are noteworthy. Inspired by Detection Transformer, which excels in object detection, we view scene graph generation as a set prediction problem. In this paper, we propose an end-to-end scene graph generation model Relation Transformer ({RelTR}), which has an encoder-decoder architecture. The encoder reasons about the visual feature context while the decoder infers a ﬁxed-size set of triplets subject-predicate-object using different types of attention mechanisms with coupled subject and object queries. We design a set prediction loss performing the matching between the ground truth and predicted triplets for the end-to-end training. In contrast to most existing scene graph generation methods, {RelTR} is a one-stage method that predicts sparse scene graphs directly only using visual appearance without combining entities and labeling all possible predicates. Extensive experiments on the Visual Genome, Open Images V6, and {VRD} datasets demonstrate the superior performance and fast inference of our model.},
	number = {{arXiv}:2201.11460},
	publisher = {{arXiv}},
	author = {Cong, Yuren and Yang, Michael Ying and Rosenhahn, Bodo},
	urldate = {2025-12-09},
	date = {2023-04-14},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2201.11460 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:/Users/tamurasae/Zotero/storage/6YBTRYHF/Cong et al. - 2023 - RelTR Relation Transformer for Scene Graph Generation.pdf:application/pdf},
}

@misc{egtr,
	title = {{EGTR}: Extracting Graph from Transformer for Scene Graph Generation},
	url = {http://arxiv.org/abs/2404.02072},
	doi = {10.48550/arXiv.2404.02072},
	shorttitle = {{EGTR}},
	abstract = {Scene Graph Generation ({SGG}) is a challenging task of detecting objects and predicting relationships between objects. After {DETR} was developed, one-stage {SGG} models based on a one-stage object detector have been actively studied. However, complex modeling is used to predict the relationship between objects, and the inherent relationship between object queries learned in the multi-head self-attention of the object detector has been neglected. We propose a lightweight one-stage {SGG} model that extracts the relation graph from the various relationships learned in the multi-head self-attention layers of the {DETR} decoder. By fully utilizing the self-attention by-products, the relation graph can be extracted effectively with a shallow relation extraction head. Considering the dependency of the relation extraction task on the object detection task, we propose a novel relation smoothing technique that adjusts the relation label adaptively according to the quality of the detected objects. By the relation smoothing, the model is trained according to the continuous curriculum that focuses on object detection task at the beginning of training and performs multi-task learning as the object detection performance gradually improves. Furthermore, we propose a connectivity prediction task that predicts whether a relation exists between object pairs as an auxiliary task of the relation extraction. We demonstrate the effectiveness and efficiency of our method for the Visual Genome and Open Image V6 datasets. Our code is publicly available at https://github.com/naver-ai/egtr.},
	number = {{arXiv}:2404.02072},
	publisher = {{arXiv}},
	author = {Im, Jinbae and Nam, {JeongYeon} and Park, Nokyung and Lee, Hyungmin and Park, Seunghyun},
	urldate = {2025-12-09},
	date = {2024-06-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2404.02072 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {PDF:/Users/tamurasae/Zotero/storage/Y6QGEJX2/Im et al. - 2024 - EGTR Extracting Graph from Transformer for Scene Graph Generation.pdf:application/pdf},
}

@misc{zhao_survey_2025,
	title = {A Survey of Large Language Models},
	url = {http://arxiv.org/abs/2303.18223},
	doi = {10.48550/arXiv.2303.18223},
	abstract = {Ever since the Turing Test was proposed in the 1950s, humans have explored the mastering of language intelligence by machine. Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable artificial intelligence ({AI}) algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models ({PLMs}) have been proposed by pretraining Transformer models over large-scale corpora, showing strong capabilities in solving various natural language processing ({NLP}) tasks. Since the researchers have found that model scaling can lead to an improved model capacity, they further investigate the scaling effect by increasing the parameter scale to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement, but also exhibit some special abilities (e.g., incontext learning) that are not present in small-scale language models (e.g., {BERT}). To discriminate the language models in different parameter scales, the research community has coined the term large language models ({LLM}) for the {PLMs} of significant size (e.g., containing tens or hundreds of billions of parameters). Recently, the research on {LLMs} has been largely advanced by both academia and industry, and a remarkable progress is the launch of {ChatGPT} (a powerful {AI} chatbot developed based on {LLMs}), which has attracted widespread attention from society. The technical evolution of {LLMs} has been making an important impact on the entire {AI} community, which would revolutionize the way how we develop and use {AI} algorithms. Considering this rapid technical progress, in this survey, we review the recent advances of {LLMs} by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of {LLMs}, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Furthermore, we also summarize the available resources for developing {LLMs} and discuss the remaining issues for future directions. This survey provides an up-to-date review of the literature on {LLMs}, which can be a useful resource for both researchers and engineers.},
	number = {{arXiv}:2303.18223},
	publisher = {{arXiv}},
	author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
	urldate = {2025-12-10},
	date = {2025-03-11},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2303.18223 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {PDF:/Users/tamurasae/Zotero/storage/TF4CF739/Zhao et al. - 2025 - A Survey of Large Language Models.pdf:application/pdf},
}

@article{gpt-2,
	title = {Language Models are Unsupervised Multitask Learners},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called {WebText}. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the {CoQA} dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, {GPT}-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts {WebText}. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	langid = {english},
	file = {PDF:/Users/tamurasae/Zotero/storage/5L2RLDHK/Radford et al. - Language Models are Unsupervised Multitask Learners.pdf:application/pdf},
}

@misc{devlin_bert_2019,
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), {BERT} is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	number = {{arXiv}:1810.04805},
	publisher = {{arXiv}},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	urldate = {2025-12-10},
	date = {2019-05-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/Users/tamurasae/Zotero/storage/2R5KEDLF/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:application/pdf},
}

@misc{transformer,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	number = {{arXiv}:1706.03762},
	publisher = {{arXiv}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2025-12-10},
	date = {2023-08-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1706.03762 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {PDF:/Users/tamurasae/Zotero/storage/SNV9R6RD/Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf},
}

@misc{touvron_llama_2023,
	title = {{LLaMA}: Open and Efficient Foundation Language Models},
	url = {http://arxiv.org/abs/2302.13971},
	doi = {10.48550/arXiv.2302.13971},
	shorttitle = {{LLaMA}},
	abstract = {We introduce {LLaMA}, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, {LLaMA}-13B outperforms {GPT}-3 (175B) on most benchmarks, and {LLaMA}65B is competitive with the best models, Chinchilla-70B and {PaLM}-540B. We release all our models to the research community1.},
	number = {{arXiv}:2302.13971},
	publisher = {{arXiv}},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	urldate = {2025-12-10},
	date = {2023-02-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2302.13971 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/Users/tamurasae/Zotero/storage/KNKX2TPF/Touvron et al. - 2023 - LLaMA Open and Efficient Foundation Language Models.pdf:application/pdf},
}

@misc{chowdhery_palm_2022,
	title = {{PaLM}: Scaling Language Modeling with Pathways},
	url = {http://arxiv.org/abs/2204.02311},
	doi = {10.48550/arXiv.2204.02311},
	shorttitle = {{PaLM}},
	abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-speciﬁc training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model ({PaLM}).},
	number = {{arXiv}:2204.02311},
	publisher = {{arXiv}},
	author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
	urldate = {2025-12-10},
	date = {2022-10-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2204.02311 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/Users/tamurasae/Zotero/storage/I5X2CZEL/Chowdhery et al. - 2022 - PaLM Scaling Language Modeling with Pathways.pdf:application/pdf},
}

@misc{gpt-3,
	title = {Language Models are Few-Shot Learners},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many {NLP} tasks and benchmarks by pre-training on a large corpus of text followed by ﬁne-tuning on a speciﬁc task. While typically task-agnostic in architecture, this method still requires task-speciﬁc ﬁne-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions – something which current {NLP} systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art ﬁnetuning approaches. Speciﬁcally, we train {GPT}-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, {GPT}-3 is applied without any gradient updates or ﬁne-tuning, with tasks and few-shot demonstrations speciﬁed purely via text interaction with the model. {GPT}-3 achieves strong performance on many {NLP} datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-ﬂy reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where {GPT}-3’s few-shot learning still struggles, as well as some datasets where {GPT}-3 faces methodological issues related to training on large web corpora. Finally, we ﬁnd that {GPT}-3 can generate samples of news articles which human evaluators have difﬁculty distinguishing from articles written by humans. We discuss broader societal impacts of this ﬁnding and of {GPT}-3 in general.},
	number = {{arXiv}:2005.14165},
	publisher = {{arXiv}},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and {McCandlish}, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	urldate = {2025-12-10},
	date = {2020-07-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/Users/tamurasae/Zotero/storage/KDKXRM4S/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf},
}

@article{gpt-1,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:49313245}
}

@misc{gpt-4,
    title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{gpt-4o,
  title={Gpt-4o system card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}

@misc{detr,
      title={End-to-End Object Detection with Transformers}, 
      author={Nicolas Carion and Francisco Massa and Gabriel Synnaeve and Nicolas Usunier and Alexander Kirillov and Sergey Zagoruyko},
      year={2020},
      eprint={2005.12872},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2005.12872}, 
}

@misc{VisualGenome,
      title={Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations}, 
      author={Ranjay Krishna and Yuke Zhu and Oliver Groth and Justin Johnson and Kenji Hata and Joshua Kravitz and Stephanie Chen and Yannis Kalantidis and Li-Jia Li and David A. Shamma and Michael S. Bernstein and Fei-Fei Li},
      year={2016},
      eprint={1602.07332},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1602.07332}, 
}

@misc{ovsgtr,
      title={Expanding Scene Graph Boundaries: Fully Open-vocabulary Scene Graph Generation via Visual-Concept Alignment and Retention}, 
      author={Zuyao Chen and Jinlin Wu and Zhen Lei and Zhaoxiang Zhang and Changwen Chen},
      year={2024},
      eprint={2311.10988},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2311.10988}, 
}

@misc{pgsg,
      title={From Pixels to Graphs: Open-Vocabulary Scene Graph Generation with Vision-Language Models}, 
      author={Rongjie Li and Songyang Zhang and Dahua Lin and Kai Chen and Xuming He},
      year={2024},
      eprint={2404.00906},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2404.00906}, 
}

@misc{sgtr,
      title={SGTR: End-to-end Scene Graph Generation with Transformer}, 
      author={Rongjie Li and Songyang Zhang and Xuming He},
      year={2022},
      eprint={2112.12970},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2112.12970}, 
}

@misc{clip,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.00020}, 
}

@misc{groundingdino,
      title={Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection}, 
      author={Shilong Liu and Zhaoyang Zeng and Tianhe Ren and Feng Li and Hao Zhang and Jie Yang and Qing Jiang and Chunyuan Li and Jianwei Yang and Hang Su and Jun Zhu and Lei Zhang},
      year={2024},
      eprint={2303.05499},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2303.05499}, 
}

@misc{blip,
      title={BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation}, 
      author={Junnan Li and Dongxu Li and Caiming Xiong and Steven Hoi},
      year={2022},
      eprint={2201.12086},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2201.12086}, 
}

@misc{instructblip,
      title={InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning}, 
      author={Wenliang Dai and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi},
      year={2023},
      eprint={2305.06500},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2305.06500}, 
}